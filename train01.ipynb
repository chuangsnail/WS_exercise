{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28777de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import sklearn\n",
    "import math\n",
    "import joblib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d6e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the npz file\n",
    "train_1 = np.load(\"data/train_data.npy\")\n",
    "train_2 = np.load(\"data/train_labels.npy\")\n",
    "train_df = DataFrame(train_1)\n",
    "del train_1\n",
    "train_lb_df = DataFrame(train_2)\n",
    "del train_2\n",
    "\n",
    "# For validation\n",
    "val_1 = np.load(\"data/eval_data.npy\")\n",
    "val_2 = np.load(\"data/eval_labels.npy\")\n",
    "val_df = DataFrame(val_1)\n",
    "del val_1\n",
    "val_lb_df = DataFrame(val_2)\n",
    "del val_2\n",
    "\n",
    "# for unit_test\n",
    "\n",
    "uni_df = train_df.iloc[100:,:]\n",
    "uni_lb_df = train_lb_df.iloc[100:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a1c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(val_lb_df.iloc[:,0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0c8f93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2]\n",
      "class 0 :  0.3333333333333333\n",
      "class 1 :  0.3333333333333333\n",
      "class 2 :  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(train_lb_df.iloc[:,0].unique()) #[1, 0, 2]\n",
    "# Draw some plots to simply see the characteristics of training data\n",
    "def DrawDist( ser_ft, ser_lb, ps=\"\" ):\n",
    "    ft_name = str(ser_ft.name)\n",
    "    fig = plt.figure(figsize=(4,5), dpi=70)\n",
    "    plt.hist(ser_ft[ser_lb==0], color='red', cumulative=False, \n",
    "             alpha=0.3,histtype='step', density=True)\n",
    "    plt.hist(ser_ft[ser_lb==1], color='blue', cumulative=False, \n",
    "             alpha=0.3,histtype='step', density=True)\n",
    "    plt.hist(ser_ft[ser_lb==2], color='grey', cumulative=False, \n",
    "             alpha=0.3,histtype='step', density=True)\n",
    "    plt.show()\n",
    "    fig.savefig(\"Fig/\" + ft_name + ps + \"_label.png\")\n",
    "\n",
    "#for ft in train_df.columns:\n",
    "#   DrawDist( train_df[ft], train_lb_df.iloc[:,0] )\n",
    "\n",
    "#train_lb_df = pd.Series(train_lb_df.iloc[:,0])\n",
    "# Check if the classes are imbalance\n",
    "no_0 = len(train_lb_df[train_lb_df==0])\n",
    "no_1 = len(train_lb_df[train_lb_df==1])\n",
    "no_2 = len(train_lb_df[train_lb_df==2])\n",
    "print(\"class 0 : \", no_0/(no_0+no_1+no_2))\n",
    "print(\"class 1 : \", no_1/(no_0+no_1+no_2))\n",
    "print(\"class 2 : \", no_2/(no_0+no_1+no_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e791e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:56, categories #:2 [0. 1.]\n",
      "feature:57, categories #:2 [0. 1.]\n",
      "feature:58, categories #:2 [0. 1.]\n",
      "feature:59, categories #:2 [0. 1.]\n",
      "feature:60, categories #:2 [0. 1.]\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for ft in train_df.columns:\n",
    "    ft_uni_list = train_df[ft].unique()\n",
    "    if len(ft_uni_list) < 100: #if len(ft_uni_list) < 10:\n",
    "        print(\"feature:\" + str(ft) + \", categories #:\" + str(len(ft_uni_list)), ft_uni_list )\n",
    "# deal with the categories feature => they have been already one-hot\n",
    "print(type(train_df.columns[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e8de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardlize the datas\n",
    "#to check if the features have already standardlized\n",
    "\"\"\"\n",
    "for ft in train_df.columns:\n",
    "    print(str(ft) + \"'s mean:\", train_df[ft].mean())\n",
    "    print(str(ft) + \"'s std:\", train_df[ft].std())\n",
    "\"\"\"\n",
    "\n",
    "def StandardlizeData(df, continuous_list=[]):\n",
    "    if len(continuous_list) == 0:\n",
    "        continuous_list = df.columns\n",
    "    from sklearn import preprocessing\n",
    "    scaler = preprocessing.StandardScaler().fit( df[continuous_list] )\n",
    "    df[continuous_list] = scaler.transform( df[continuous_list] )\n",
    "    \n",
    "    return df, scaler\n",
    "# Except the binary features, others have been already standardlized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for [0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea83154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchingBDT( X, y, _scoring=\"accuracy\" ):\n",
    "    from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "    from sklearn.ensemble import AdaBoostClassifier as AdaB\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV as GS\n",
    "\n",
    "    param = { \"base_estimator__max_features\" : [\"sqrt\", 0.7, None], \"base_estimator__max_leaf_nodes\" : [100, None], \n",
    "              \"base_estimator__max_depth\" : range(4,28,2), \"n_estimators\": [30, 50, 70] } #for f1\n",
    "    tree = DTC( random_state=1 )\n",
    "    ABC = AdaB( base_estimator=tree, random_state=11, learning_rate=0.1 )\n",
    "    grid_search_ABC = GS(ABC, param_grid=param, scoring = _scoring, verbose=10, cv=3, n_jobs=-1)\n",
    "    grid_search_ABC.fit(X, y)\n",
    "    print(grid_search_ABC.best_score_, grid_search_ABC.best_estimator_, grid_search_ABC.best_params_)\n",
    "    # Store the best model\n",
    "    final_model = grid_search_ABC.best_estimator_\n",
    "    model_name = \"Model/GS_01_BDT_training_best_\" + _scoring\n",
    "    joblib.dump( final_model, model_name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39ddf2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1174461, step=1)\n",
      "(803, 70)\n",
      "(8028, 70)\n"
     ]
    }
   ],
   "source": [
    "print(train_lb_df.index)\n",
    "print((train_df.iloc[:1000,:].loc[(train_lb_df[0]==0)|(train_lb_df[0]==1),:]).shape)\n",
    "print((train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<10000),:]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7631467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "0.6990533134030891 AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=12,\n",
      "                                                         max_features='sqrt',\n",
      "                                                         random_state=1),\n",
      "                   learning_rate=0.1, n_estimators=70, random_state=11) {'base_estimator__max_depth': 12, 'base_estimator__max_features': 'sqrt', 'base_estimator__max_leaf_nodes': None, 'n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "GridSearchingBDT( train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<10000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<10000),0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5590988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BDTTraining(X, y, name=\"\"):\n",
    "    from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "    from sklearn.ensemble import AdaBoostClassifier as AdaB\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "    tree = DTC( random_state=1, max_depth=12, max_features=\"sqrt\", max_leaf_nodes=None )\n",
    "    ABC = AdaB( base_estimator=tree, random_state=11, learning_rate=0.1, n_estimators=70 )\n",
    "    ABC.fit(X, y)\n",
    "    # Store the best model\n",
    "    final_model = ABC\n",
    "    model_name = \"Model/01_BDT_training_\" + name\n",
    "    joblib.dump( final_model, model_name )\n",
    "    print(\"Accuracy : \", accuracy_score(val_lb_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:],  final_model.predict(val_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6230e932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7204341317365269\n"
     ]
    }
   ],
   "source": [
    "BDTTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),0]\n",
    "                  , \"12sqrtnone70\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42dfc115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchingRF( X, y, _scoring=\"accuracy\" ):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV as GS\n",
    "\n",
    "    param = { \"n_estimators\" : [100, 150, 200, 250, 300, 350, 400], \"max_features\":[\"sqrt\", 0.7, None] } \n",
    "    forest = RandomForestClassifier( criterion='gini', random_state=22, n_jobs=-1)\n",
    "    grid_search = GS(forest, param_grid=param, scoring = _scoring, verbose=10, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    print(grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_)\n",
    "    # Store the best model\n",
    "    final_model = grid_search.best_estimator_\n",
    "    model_name = \"Model/GS_01_RF_training_best_\" + _scoring\n",
    "    joblib.dump( final_model, model_name )\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "842bd930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 21 candidates, totalling 63 fits\n",
      "0.6948181365221724 RandomForestClassifier(max_features='sqrt', n_estimators=350, n_jobs=-1,\n",
      "                       random_state=22) {'max_features': 'sqrt', 'n_estimators': 350}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features='sqrt', n_estimators=350, n_jobs=-1,\n",
       "                       random_state=22)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchingRF( train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<10000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<10000),0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2af1058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFTraining(X, y, name=\"\"):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV as GS\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "    forest = RandomForestClassifier( criterion='gini', n_estimators=350, max_features=None, random_state=22, n_jobs=-1)\n",
    "    forest.fit(X, y)\n",
    "    # Store the best model\n",
    "    final_model = forest\n",
    "    model_name = \"Model/01_RF_training_\" + name\n",
    "    joblib.dump( final_model, model_name )\n",
    "    print(\"Accuracy : \", accuracy_score(val_lb_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:],  final_model.predict(val_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33e23b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7287558568663278\n"
     ]
    }
   ],
   "source": [
    "RFTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),0]\n",
    "                  , \"25none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb568ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7300787398013641\n"
     ]
    }
   ],
   "source": [
    "RFTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),0]\n",
    "                  , \"25sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cf9844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.742245870796172\n"
     ]
    }
   ],
   "source": [
    "RFTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),0]\n",
    "                  , \"350sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd9d4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7440707708450169\n"
     ]
    }
   ],
   "source": [
    "RFTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),0]\n",
    "                  , \"350none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66af75d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.742245870796172\n"
     ]
    }
   ],
   "source": [
    "RFTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),0]\n",
    "                  , \"150sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f307b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchingLR( X, y, _scoring=\"accuracy\" ):\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV as GS\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    grid_search = GS(LogisticRegression(), param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 10., 100.]}, cv=2, refit=True, scoring = _scoring, verbose=10, n_jobs=-1)\n",
    "    pipeline = make_pipeline(StandardScaler(), grid_search)\n",
    "    \n",
    "    pipeline.fit(X, y)\n",
    "    print(grid_search.best_score_, grid_search.best_estimator_, grid_search.best_params_)\n",
    "    # Store the best model\n",
    "    final_model = grid_search.best_estimator_\n",
    "    model_name = \"Model/GS_01_LR_training_best_\" + _scoring\n",
    "    joblib.dump( final_model, model_name )\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62e5eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "0.7445564559440297 LogisticRegression(C=0.01) {'C': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchingLR( train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<100000),0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b44f6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRTraining(X, y, name=\"\"):\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "    lr = LogisticRegression(C=0.001)\n",
    "    pipeline = make_pipeline(StandardScaler(), lr)\n",
    "    pipeline.fit(X, y)\n",
    "    # Store the best model\n",
    "    final_model = pipeline\n",
    "    model_name = \"Model/01_final_LR_training_\" + name\n",
    "    joblib.dump( final_model, model_name )\n",
    "    print(\"Accuracy : \", accuracy_score(val_lb_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:],  final_model.predict(val_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:])))\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd6f2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7401993595889791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression(C=0.01))])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),0]\n",
    "                  , \"0pt01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5a763b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7403780053186678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression(C=0.001))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),0]\n",
    "                  , \"0pt001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad85ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\snail\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7402570237169166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression(C=0.1))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),0]\n",
    "                  , \"0pt1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c351af14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\snail\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7402491090326899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index<300000),0]\n",
    "                  , \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "133a7cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7398952095808383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression(C=0.001))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRTraining(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index>900000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index>900000),0]\n",
    "                  , \"0pt001_other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b2773",
   "metadata": {},
   "source": [
    "### Do the model ensembling for [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "21636fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_EnsembleModel(X, y, name=\"\"):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "    \n",
    "    lr = LogisticRegression(C=0.001)\n",
    "    pipeline = make_pipeline(StandardScaler(), lr)\n",
    "    \n",
    "    forest = RandomForestClassifier( n_estimators=350, criterion='gini', random_state=22, n_jobs=-1, max_features=None )\n",
    "    \n",
    "    tree = DecisionTreeClassifier( random_state=1, max_depth=12, max_features=\"sqrt\", max_leaf_nodes=None )\n",
    "    ABC = AdaBoostClassifier( base_estimator=tree, random_state=11, learning_rate=0.1, n_estimators=70 )\n",
    "    \n",
    "    model = VotingClassifier(estimators=[\n",
    "            ('lr', pipeline), ('rf', forest), ('bdt', ABC)],\n",
    "            voting='soft')\n",
    "    model = model.fit(X, y)\n",
    "    joblib.dump( model, \"01_final_Ensemble_training_\"+name )\n",
    "    print(\"Accuracy : \", accuracy_score(val_lb_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:],  model.predict(val_df.loc[(val_lb_df[0]==0)|(val_lb_df[0]==1),:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40b3a897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7414363116666968\n"
     ]
    }
   ],
   "source": [
    "Training_EnsembleModel(train_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index>1000000),:],\n",
    "                  train_lb_df.loc[((train_lb_df[0]==0)|(train_lb_df[0]==1)) & (train_lb_df.index>1000000),0]\n",
    "                  , \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57bb15a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174461\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lb_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ccf522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.725 +/- 0.007\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_0_1 = RandomForestClassifier(criterion='gini',\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    "    n_jobs=-1)\n",
    "scores_RF_0_1_all = cross_val_score(forest_0_1, train_df.loc[(train_lb_df[0]==0)|(train_lb_df[0]==1),:],\n",
    "                         train_lb_df.loc[(train_lb_df[0]==0)|(train_lb_df[0]==1),0], scoring='accuracy', cv=5)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_RF_0_1_all), np.std(scores_RF_0_1_all)))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
